# -*- coding: utf-8 -*-
"""text_lab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJnATclDw_R9Lly712nrw3sXtq9CKc6Z

1.Застосувати приховане семантичне індексування бібліотеки scikit-learn для моделювання тем. Вивести документи, що описують кожну з тем. Вивести найбільш важливі теми для випадково обраних чотирьох документів.
2. Використати текст carroll-alice.txt з корпусу gutenberg бібліотеки nltk та вивести ключові біграми.

**Завантаження файлу у датафрейм**
"""

import pandas as pd

news_corpus = pd.read_csv('news.csv', sep='\t')
news_corpus

"""**Попередня обробка**"""

from nltk.tokenize import WordPunctTokenizer
from nltk.corpus import stopwords
import re
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
tokenizer = WordPunctTokenizer()


def preprocess_document(doc):
    doc = re.sub(r'https?:\/\/\S+', '', doc)  # Видаляємо посилання
    doc = re.sub(r'[^a-zA-Z\s]', '', doc, re.I | re.A)
    doc = doc.lower()
    doc = doc.strip()
    tokens = tokenizer.tokenize(doc)
    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
    doc = ' '.join(filtered_tokens)
    return doc

# Застосовуємо функцію до всіх документів у корпусі
news_corpus['processed_text'] = news_corpus['text,label'].apply(preprocess_document)
news_corpus['processed_text'].head()

"""**Використання моделі TF-IDF та застосування прихованого семантичного індексування**"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

cv = TfidfVectorizer(min_df=10, max_df=0.8, ngram_range=(1, 2))
cv_features = cv.fit_transform(news_corpus['processed_text'])
vocabulary = np.array(cv.get_feature_names_out())
from sklearn.decomposition import TruncatedSVD

n_topics = 10
lsi_model = TruncatedSVD(n_components=n_topics, random_state=1234)
document_topics = lsi_model.fit_transform(cv_features)
document_topics

"""**Вивести документи, що описують кожну з тем**"""

topic_docs = {}
for i, topic in enumerate(lsi_model.components_):
    top_docs_idx = np.argsort(topic)[::-1][:5]  # Вибираємо топ-5 документів для кожної теми
    topic_docs[i] = top_docs_idx

for topic, docs_idx in topic_docs.items():
    print(f"Тема {topic}:")
    for doc_idx in docs_idx:
        print(news_corpus.iloc[doc_idx]['processed_text'])
    print()

"""**Найбільш важливі теми для випадково обраних чотирьох документів**"""

top_topics = 5
documents_indices = np.random.choice(document_topics.shape[0], size=4, replace=False)
random_documents = document_topics[documents_indices]
most_important_topics = np.argsort(np.abs(random_documents), axis=1)[:, ::-1][:, :top_topics]
most_important_topics = pd.DataFrame(most_important_topics, index=[news_corpus['processed_text'][documents_indices]],
                                     columns=[f'Top-{i + 1}' for i in range(top_topics)])
most_important_topics

"""Використати текст carroll-alice.txt з корпусу gutenberg бібліотеки nltk та вивести ключові біграми."""

from nltk.corpus import gutenberg
nltk.download('gutenberg')
nltk.download('punkt')

alice_corpus = gutenberg.sents('carroll-alice.txt')
def preprocess_sentence(sentence):
    doc = ' '.join(sentence)
    doc = re.sub(r'[^a-zA-Z\s]', '', doc, re.I | re.A)
    doc = doc.lower()
    doc = doc.strip()
    tokens = tokenizer.tokenize(doc)
    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
    return filtered_tokens
alice_corpus = [preprocess_sentence(sentence) for sentence in alice_corpus]
print(alice_corpus[:5])

"""Топ-5 ключових біграм"""

from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures

bigram_measures = BigramAssocMeasures()
finder = BigramCollocationFinder.from_documents(alice_corpus)
finder.apply_freq_filter(5)
finder.nbest(bigram_measures.pmi, 5)